# ğŸ“ DevSecOps AI - Project Structure Overview

## ğŸ“ Directory Structure

```
devsecopsai/
â”œâ”€â”€ ğŸ“„ README.md                      # Main project documentation
â”œâ”€â”€ ğŸ“„ PROJECT_GUIDE.md               # Academic deliverables guide
â”œâ”€â”€ ğŸ“„ main.py                        # CLI entry point
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example                   # Environment variables template
â”œâ”€â”€ ğŸ“„ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“‚ pipelines/                     # CI/CD configurations
â”‚   â”œâ”€â”€ .gitlab-ci.yml               # GitLab CI pipeline
â”‚   â””â”€â”€ .github-actions.yml          # GitHub Actions workflow
â”‚
â”œâ”€â”€ ğŸ“‚ scanners/                      # Security scanning tools
â”‚   â”œâ”€â”€ scanner_orchestrator.py     # Main coordinator
â”‚   â”œâ”€â”€ sast/                        # Static analysis
â”‚   â”‚   â”œâ”€â”€ bandit_scanner.py
â”‚   â”‚   â””â”€â”€ sonarqube_scanner.py
â”‚   â”œâ”€â”€ sca/                         # Dependency scanning
â”‚   â”‚   â”œâ”€â”€ dependency_check_scanner.py
â”‚   â”‚   â””â”€â”€ safety_scanner.py
â”‚   â””â”€â”€ dast/                        # Dynamic analysis
â”‚       â””â”€â”€ zap_scanner.py
â”‚
â”œâ”€â”€ ğŸ“‚ parsers/                       # Report parsing
â”‚   â””â”€â”€ report_parser.py             # Unified parser for all tools
â”‚
â”œâ”€â”€ ğŸ“‚ llm_engine/                    # LLM integration
â”‚   â”œâ”€â”€ llm_manager.py               # Multi-provider manager
â”‚   â””â”€â”€ prompt_engine.py             # Prompt templates
â”‚
â”œâ”€â”€ ğŸ“‚ policy_generator/              # Policy creation
â”‚   â””â”€â”€ policy_orchestrator.py       # Policy generation logic
â”‚
â”œâ”€â”€ ğŸ“‚ evaluation/                    # Quality assessment
â”‚   â””â”€â”€ evaluator.py                 # BLEU, ROUGE-L, compliance
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          # Sample data
â”‚   â”œâ”€â”€ reports/                     # Sample vulnerability reports
â”‚   â””â”€â”€ reference_policies/          # Reference policy templates
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                          # Documentation
â”‚   â”œâ”€â”€ setup.md                     # Setup instructions
â”‚   â”œâ”€â”€ quickstart.md                # Quick start guide
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                         # Test suite
â”‚   â”œâ”€â”€ unit/                        # Unit tests
â”‚   â””â”€â”€ integration/                 # Integration tests
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                       # Utility scripts
â”‚   â”œâ”€â”€ setup.sh                     # Quick setup
â”‚   â”œâ”€â”€ run_tests.sh                 # Test runner
â”‚   â””â”€â”€ generate_final_report.py    # Report generator
â”‚
â”œâ”€â”€ ğŸ“‚ sample_app/                    # Vulnerable test app
â”‚   â”œâ”€â”€ app.py                       # Flask app with vulnerabilities
â”‚   â””â”€â”€ requirements.txt             # Vulnerable dependencies
â”‚
â””â”€â”€ ğŸ“‚ output/                        # Generated files (gitignored)
    â”œâ”€â”€ generated_policies/          # AI-generated policies
    â””â”€â”€ evaluation_results/          # Evaluation metrics
```

## ğŸ¯ Key Components

### 1. **Security Scanners** (`scanners/`)
- **SAST**: Bandit, SonarQube
- **SCA**: OWASP Dependency-Check, Safety
- **DAST**: OWASP ZAP
- Orchestrator coordinates all scanners

### 2. **Report Parser** (`parsers/`)
- Unified interface for all security tools
- Normalizes vulnerability data
- Supports JSON, XML, HTML formats

### 3. **LLM Engine** (`llm_engine/`)
- Multi-provider support:
  - OpenAI (GPT-4, GPT-3.5)
  - Anthropic (Claude)
  - Ollama (Local models)
  - DeepSeek (R1)
  - Hugging Face (Transformers)
- Framework-specific prompt templates
- Retry logic and error handling

### 4. **Policy Generator** (`policy_generator/`)
- Generates policies from vulnerabilities
- Supports multiple frameworks:
  - NIST Cybersecurity Framework
  - ISO/IEC 27001
  - CIS Controls
- Category-specific policies
- Refinement capabilities

### 5. **Evaluator** (`evaluation/`)
- **BLEU**: N-gram overlap
- **ROUGE-L**: Longest common subsequence
- **Compliance**: Framework requirement coverage
- **Readability**: Flesch reading ease
- Generates detailed reports

## ğŸš€ Quick Start Commands

```bash
# 1. Setup
./scripts/setup.sh

# 2. Configure
nano .env  # Add your API keys

# 3. Test
python main.py check-config

# 4. Scan
python main.py scan --target ./sample_app

# 5. Generate Policies
python main.py generate \
  --input ./data/reports \
  --framework NIST_CSF

# 6. Evaluate
python main.py evaluate \
  --policies ./output/generated_policies \
  --reference ./data/reference_policies
```

## ğŸ“Š Research Workflow

### Phase 1: Data Collection
```bash
# Scan multiple targets
python main.py scan --target ./sample_app
python main.py scan --target /path/to/real/project
```

### Phase 2: Policy Generation (Comparative Study)
```bash
# Try different LLMs
LLM_PROVIDER=openai LLM_MODEL=gpt-4 python main.py generate ...
LLM_PROVIDER=openai LLM_MODEL=gpt-3.5-turbo python main.py generate ...
LLM_PROVIDER=anthropic python main.py generate ...
LLM_PROVIDER=ollama python main.py generate ...
```

### Phase 3: Evaluation
```bash
# Calculate all metrics
python main.py evaluate \
  --policies ./output/generated_policies \
  --reference ./data/reference_policies \
  --metrics BLEU,ROUGE-L,COMPLIANCE,READABILITY
```

### Phase 4: Report Generation
```bash
# Create final academic report
python scripts/generate_final_report.py \
  --evaluation ./output/evaluation_results \
  --policies ./output/generated_policies \
  --output ./reports/final_report.pdf
```

## ğŸ“š Documentation

- **README.md**: Project overview and features
- **PROJECT_GUIDE.md**: Academic deliverables checklist
- **docs/setup.md**: Detailed setup instructions
- **docs/quickstart.md**: 5-minute quick start
- **CONTRIBUTING.md**: Contribution guidelines

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov --cov-report=html

# Specific tests
pytest tests/unit/test_parser.py
```

## ğŸ“ˆ Expected Outputs

### Generated Policies
- `output/generated_policies/*.json` - Policy documents
- `output/generated_policies/*.md` - Readable format

### Evaluation Results
- `output/evaluation_results/summary.json` - Metrics
- `output/evaluation_results/evaluation_report.md` - Analysis

### Final Report
- `reports/final_report.md` - Comprehensive report
- `reports/final_report.pdf` - Academic submission

## ğŸ“ Academic Deliverables

1. âœ… **Technical Implementation** (25%)
   - All components implemented
   - CI/CD pipelines configured
   - Multi-tool integration

2. âœ… **Research Components** (20%)
   - LLM comparison framework
   - Evaluation metrics
   - Analysis tools

3. âœ… **Policy Quality** (20%)
   - Multiple metrics
   - Standards compliance
   - Automated assessment

4. ğŸ“ **Report** (15%)
   - Use generated report as template
   - Add your analysis
   - Include visualizations

5. ğŸ¤ **Presentation** (20%)
   - Demo the system
   - Show results
   - Discuss findings

## ğŸ’¡ Tips for Success

### Technical
- Start with sample data
- Use Ollama for development (free)
- Save all experiments
- Document configurations

### Research
- Compare at least 3 LLMs
- Run multiple evaluations
- Create comparison tables
- Visualize results

### Report Writing
- Follow academic structure
- Include all metrics
- Discuss limitations
- Address ethics

## ğŸ”— Key Files to Understand

1. **main.py** - Start here, CLI interface
2. **llm_engine/llm_manager.py** - LLM integration
3. **policy_generator/policy_orchestrator.py** - Policy creation
4. **evaluation/evaluator.py** - Metrics calculation
5. **pipelines/.gitlab-ci.yml** - CI/CD example

## ğŸ“ Getting Help

- Read documentation in `docs/`
- Check sample files in `data/`
- Review test cases in `tests/`
- Examine code comments

## âœ¨ What Makes This Special

1. **Multi-LLM Support**: Compare different models
2. **Standards Compliance**: NIST, ISO, CIS
3. **Quantitative Evaluation**: BLEU, ROUGE-L
4. **Production Ready**: CI/CD integration
5. **Research Oriented**: Comparative study framework
6. **Well Documented**: Comprehensive guides
7. **Extensible**: Easy to add new tools/frameworks

## ğŸ¯ Success Metrics

Your project is successful if you can:
- âœ… Scan code for vulnerabilities
- âœ… Generate policies with AI
- âœ… Evaluate policy quality
- âœ… Compare different LLMs
- âœ… Produce academic report
- âœ… Demonstrate the system

## ğŸš€ Next Steps

1. Run `./scripts/setup.sh`
2. Configure your `.env`
3. Try the quick start commands
4. Start your research!

Good luck! ğŸ“
